\documentclass[abstract=true]{scrartcl}
%set letterpaper, 10pt for american
\usepackage{amsmath, amssymb, amsthm} %necessary math packages
\usepackage{verbatim} %if you need not to interpret latex
\usepackage{graphicx} %insert figures
\usepackage{booktabs} %nice tables
\usepackage[colorlinks]{hyperref} %links
\usepackage{xcolor} %definecolors
\usepackage{enumerate} %enumerate
\usepackage{natbib} %bib organization
\usepackage[affil-it]{authblk} %better maketitle
\usepackage{mdframed} %frame example
\usepackage{microtype} %small improvement
\usepackage{caption} %caption
\usepackage{style/matheusfarias} %my style
%\usepackage[body={4.8in,7.5in}, 
%    top=1.2in, left=1.8in]{geometry} %american page layout

\begin{document}
\title{CS242 -- Computing at Scale}
\date{Fall, 2021}

\author{Matheus S. Farias%
  \thanks{E-mail address: \href{mailto:matheusfarias@g.harvard.edu}{matheusfarias@g.harvard.edu}}}
\affil{School of Engineering and Applied Sciences, Harvard University}

\maketitle

\begin{abstract}
    This document is composed by lecture notes of CS242 -- Computing at Scale, taught by Professor H.T. Kung in Fall 2021. I am responsible to all mistakes here written.
\end{abstract}

\tableofcontents

\section{09/01}
A first and important discussion about the difference between three hyped areas. One can see a graphical perspective in Figure  \ref{fig:aivsmlvsdl}.  
\begin{itemize}
    \item \textbf{Artificial Intelligence} -- a program that can sense, reason, act and adapt.
    \item \textbf{Machine Learning} -- algorithms whose performance improve as they are exposed to more data over time.
    \item \textbf{Deep Learning} -- subset of machine learning in which multilayered neural networks learn from vast amounts of data.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width = 0.5\textwidth]{../CS242/Figures/aivsmlvsdl.pdf}
    \caption{Deep learning is a subset of machine learning, that is a subset of artificial intelligence.}
    \label{fig:aivsmlvsdl}
\end{figure}

\subsection{The Golden Age}
We are in a new golden age in the field of computer architecture. Previously if more computing resources were demanded, one could simply wait for new processors or CPUs that run faster and more efficiently. The economy also depends on the increase of computing performance to facilitate the innovation in various fields. However, the Dennard scaling and Mooreâ€™s law have almost ended. The alternative architectures, such as domain-specific accelerators, can play a role to continue scaling of performance and efficiency.

In CS242, we are interested in \textbf{scaling computations} for AI-assisted systems focusing on acceleration strategies, both \emph{speed} and \emph{efficiency}.

\subsection{Convolutional Neural Networks}

Convolutional neural network (CNN) \citep{lecun98} is one kind of neural network architecture. It uses convolutions between layers, and more common used on image processing applications due to high dimension.
In general, a neural network consists of two steps to work:

\begin{itemize}
    \item \textbf{Inference} -- A series of \textbf{inner products} (matrix multiplications), each of them is just a sequence of \emph{multiplier-accumulator (MAC)} operations.
    \item \textbf{Training} -- A more hardware demanding process, needs to take \emph{gradients}.
\end{itemize}

One of the greatest challenges in the area is the \textbf{energy consumption}. It is always a trade off choice, the higher the performance in a metric, the less it will be in another one.

To understand how CNN works, we must first learn the idea of General Matrix to Matrix Multiplication (GEMM).

\subsection{General Matrix to Matrix Multiplication}
Part of the Basic Linear Algebra Subprograms (BLAS), developed in 1979, GEMM was created to tackle matrix multiplication in \textbf{high dimensions}.
Almost all the running time of a CNN is spent on \emph{fully connected} and \emph{convolutional layers}, implemented using GEMM. Image processing applications usually work with very big matrixes (also considering RGB channels).
Big matrixes lead to big number of operations, which can result in \textbf{billions of floating point operations per second (FLOPS) on a single frame!}

Let's see how GEMM works with fully connected and convolutional layers.

\subsubsection{Fully Connected Layers}
We have an input vector with the red color (see Figure \ref{fig:gemmfc}), where it has \textbf{k} input values. In this diagram we are considering a mini-batch size (\textbf{m}) of just 1 vector.
Then we multiply the input vector to a matrix of weights (actually, it's a dot product) with \textbf{n} neurons. The output of this layer is a matrix (or vector) \textbf{m} x \textbf{n}.

Fully connected layers are more specific for classification.
\begin{figure}
    \centering
    \includegraphics[width = \textwidth]{../CS242/Figures/gemmfc.pdf}
    \caption{GEMM schematic for fully connected layers.}
    \label{fig:gemmfc}
\end{figure}

\subsubsection{Convolutional Layers}
In this case we are frequently talking about images, so our input is a three-dimensional array, and the depth is related to colors (RGB) and also opacity (RGBA).
This process consists on taking a convolution kernel, which is also a three-dimensional array, and applying it at many portions of the input image. 

So the kernel is applied to a grid of points, where each point multiplies input values and weights, and then sum.
It looks like an edge detector, once the kernel has a specific pattern, and when the input is similar to the kernel, the operation returns a high value (\textbf{inner products represent similarity}).

Convolutional layers are more specific for feature extraction.

There are some relevant variables that constitutes a CNN:

\begin{itemize}
    \item \textbf{Kernel} -- The size of the mask we are going to apply in the input image.
    \item \textbf{Padding} -- The amount of new lines filled with zero that aims to reduce the loss of dimension in the output. Try to always use odd number in the kernel to facilitate the calculation here. 
    \item \textbf{Stride} -- The slide you make after every convolution step.
    \item \textbf{Pooling} -- A kernel that do a specific operation, two common ones are max pooling and average pooling.
\end{itemize}

\emph{Padding} and \emph{stride} are used to administrate the output dimension. Assuming the input vector with dimensions $(n_H, n_W)$, the kernel with dimensions $(k_H, k_W)$, the padding with dimensions $(p_H, p_W)$, and the stride with dimensions $(s_H, s_W)$, the final output is calculated as show in Equation \ref{eq:cnnoperation}.

\begin{equation}
\left \lfloor \left(\frac{n_H + p_H - k_H + s_H}{s_H}\right)\right \rfloor \times \left \lfloor \left(\frac{n_W + p_W - k_W + s_W}{s_W}\right)\right \rfloor    
\label{eq:cnnoperation}
\end{equation}

After applying the kernel, our multiplication (without considering padding and stride) is represented in Figure \ref{fig:gemmcv2}. Note that if the stride is less than the number of kernels, we may have overlapping information between lines/columns.
At a first glance, it sounds inefficient, but it's worth it.
\begin{figure}
    \centering
    \includegraphics[width = \textwidth]{../CS242/Figures/gemmcv2.pdf}
    \caption{Final GEMM schematic for convolutional layers.}
    \label{fig:gemmcv2}
\end{figure}

\subsection{Parallelism}

The idea of scaling is to use many computers to train and perform tasks. The backward pass is \textbf{two times} more demanding than the forward. Also, it requires the activations performed by the forwarding pass, thus it needs to \textbf{store the calculations} (can be a difficult task to high dimensions).

We have mainly three different ways to do parallelization in neural networks: \emph{data parallel}, \emph{model-parallel: interlayer}, and \emph{model-parallel: intralayer}.

\subsubsection{Data-parallel}
In this model, each worker has a copy of the entire model, and each of them gets a different mini-batch. This way, the \textbf{forward and back-propagations are the same}, but the problem is the \emph{weight update}.

To deal with communication during weight update, we have two common ways:

\begin{itemize}
    \item \textbf{Ring reduction} -- each worker communicates with two other neighbors, thus it has $2(N-1)$ synchronizations total (one can think of communication with 4 neighbors, it's called toroidal and it's used in TPUs).
    \item \textbf{Fully connected} -- each worker communicates with everyone ($N-1$ neighbors), each of them with $N-1$ substeps.
\end{itemize}

\subsubsection{Model-parallel: interlayer}
Also called \textbf{pipeline parallel}, this model is challenging as it \textbf{needs communication on each stage} (forward and back). Also, the timing of passing the calculation from a previous worker to the next one can be a problem, once the next one can be \textbf{busy}.
The diagram of this method is shown in Figure \ref{fig:interlayer}.
\begin{figure}
    \centering
    \includegraphics[width = .7\textwidth]{../CS242/Figures/interlayer.pdf}
    \caption{Diagram of model-parallel: interlayer method of parallelization.}
    \label{fig:interlayer}
\end{figure}

\subsubsection{Model-parallel: intralayer}
There are various tricks in intralayer to reduce communication, such as alternating between horizontal and vertical partitions, but this approach also suffers from it being difficult to overlap communication with computation. But when the model is too big for data parallelism you have to deal with it.
The diagram of this method is shown in Figure \ref{fig:intralayer}.
\begin{figure}
    \centering
    \includegraphics[width = .7\textwidth]{../CS242/Figures/intralayer.pdf}
    \caption{Diagram of model-parallel: intralayer method of parallelization.}
    \label{fig:intralayer}
\end{figure}

\subsection{LeNet}




\bibliographystyle{apalike}
\bibliography{bib/mybib.bib} %my bib
\end{document}